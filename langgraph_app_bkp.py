"""
langgraph_app.py
LangGraph application setup for TrailAgent
"""

from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings
from langchain_qdrant import QdrantVectorStore
from langchain_core.documents import Document
from typing import TypedDict, List
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langgraph.graph import StateGraph, END
import os

# Set up NVIDIA API key (ensure this is set securely in production)
if not os.environ.get("NVIDIA_API_KEY"):
    os.environ["NVIDIA_API_KEY"] = "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"

# Chat model
chat_model = ChatNVIDIA(model="meta/llama-3.1-70b-instruct")

# Embeddings model
embeddings_model = NVIDIAEmbeddings(model="nvidia/nv-embed-v1")

# Vector store retriever (Qdrant)
def get_vectorstore_retriever():
    vector_store = QdrantVectorStore.from_existing_collection(
        collection_name="nps",
        embedding=embeddings_model,
        url="https://e08da72a-93d5-4d9b-9ba9-b49759659b78.us-west-2-0.aws.cloud.qdrant.io:6333",
        api_key="eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.CTEbTLFOZ0QJRgzJw6kT3Msf03p4TAsm2qb60zw5T4I",
        vector_name="nps-dense"
    )
    return vector_store.as_retriever()


class GraphState(TypedDict):
    """
    Represents the state of our graph.

    Attributes:
        question: The user's input question.
        documents: List of retrieved documents.
        generation: The final answer generated by the LLM.
    """
    question: str
    documents: List[Document]
    generation: str

# Example: LangGraph application class (stub)
class TrailAgentLangGraph:
    def __init__(self):
        self.chat_model = chat_model
        self.embeddings_model = embeddings_model
        self.retriever = get_vectorstore_retriever()

    def generate_response(self, question: str) -> str:
        # Build the graph
        workflow = StateGraph(GraphState)

        # 1. Define the nodes
        workflow.add_node("retrieve", self.retrieve)  # Retrieves documents
        workflow.add_node("generate", self.generate)  # Generates answer

        # 2. Define the edges (flow)
        workflow.add_edge("retrieve", "generate")
        workflow.add_edge("generate", END)

        # 3. Set the entry point
        workflow.set_entry_point("retrieve")

        # Compile the graph
        app = workflow.compile()

        # --- Run the Query ---
        final_state = app.invoke({"question": question})
        return final_state["generation"]
    
    def generate_response_stream(self, question: str):
        """
        Stream the LLM response to the UI as it is generated.
        Yields each chunk of the response as it arrives.
        """
        workflow = StateGraph(GraphState)
        workflow.add_node("retrieve", self.retrieve)
        workflow.add_node("generate", self.generate_stream)
        workflow.add_edge("retrieve", "generate")
        workflow.add_edge("generate", END)
        workflow.set_entry_point("retrieve")
        app = workflow.compile()
        # Run the query and stream the response
        for state in app.stream({"question": question}):
            # Only yield the latest generation chunk if available
            if state and state.get("generation"):
                yield state["generation"]

    def retrieve(self, state):
        """Retrieves documents from the vector store."""
        print("---RETRIEVING DOCUMENTS---")
        question = state["question"]
        documents = self.retriever.invoke(question)
        print(documents)
        return {"documents": documents, "question": question}

    def generate(self, state):
        """Generates an answer using the LLM and retrieved documents."""
        print("---GENERATING ANSWER---")
        question = state["question"]
        documents = state["documents"]
        print(documents)

        # Prompt for RAG
        prompt = ChatPromptTemplate.from_template(
            """You are an expert Q&A system on National Parks in the US. Use the following context to answer the user's question.
            If you don't know the answer, state that you cannot find the relevant information.
            Context: {context}

            Question: {question}
            """
        )

        # RAG chain using LangChain Expression Language (LCEL)
        rag_chain = (
            {"context": lambda x: "\n".join(doc.page_content for doc in x['documents']),
            "question": RunnablePassthrough()}
            | prompt
            | chat_model
        )

        # Run the generation
        response = rag_chain.invoke({"documents": documents, "question": question})
        print(response.content)

        return {"generation": response.content, "question": question, "documents": documents}

    def generate_stream(self, state):
        """
        Streams the answer using the LLM and retrieved documents.
        """
        question = state["question"]
        documents = state["documents"]
        prompt = ChatPromptTemplate.from_template(
            """You are an expert Q&A system on National Parks in the US. Use the following context to answer the user's question.\nIf you don't know the answer, state that you cannot find the relevant information.\nContext: {context}\n\nQuestion: {question}\n"""
        )
        rag_chain = (
            {"context": lambda x: "\n".join(doc.page_content for doc in x['documents']),
            "question": RunnablePassthrough()}
            | prompt
            | self.chat_model
        )
        # Use the stream method of the chain
        response_chunks = rag_chain.stream({"documents": documents, "question": question})
        full_content = ""
        for chunk in response_chunks:
            if hasattr(chunk, "content"):
                full_content += chunk.content
                yield {"generation": full_content, "question": question, "documents": documents}
            else:
                # fallback for non-object chunk
                full_content += str(chunk)
                yield {"generation": full_content, "question": question, "documents": documents}
